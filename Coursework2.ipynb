{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JGrant3101/ME4MachineLearning/blob/main/Coursework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will start by importing all the libraries which might need to be used."
      ],
      "metadata": {
        "id": "z6KH25qmc006"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CTrRPkLkcx-l"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "from sklearn import naive_bayes\n",
        "from sklearn import linear_model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas\n",
        "import plotly.graph_objects as go\n",
        "import sys\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly going to build the neural network for dataset1"
      ],
      "metadata": {
        "id": "ngshmKkneNDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by importing the dataset\n",
        "dataset1 = pandas.read_csv('/content/dataset1.csv')\n",
        "\n",
        "# Write each column of the file to an array to make it easier to work with\n",
        "ArmLength1 = np.array(dataset1['Arm length (m)'][:])\n",
        "BallWeight1 = np.array(dataset1['Ball weight (kg)'][:])\n",
        "BallRadius1 = np.array(dataset1['Ball radius (mm)'][:])\n",
        "AirTemp1 = np.array(dataset1['Air temperature (deg C)'][:])\n",
        "SpringConst1 = np.array(dataset1['Spring constant (N per m)'][:])\n",
        "DeviceWeight1 = np.array(dataset1['Device weight (kg)'][:])\n",
        "TargetHit1 = np.array(dataset1['Target hit'][:])\n",
        "# Converting our currently 1D y values to 2D to match our model\n",
        "TargetHit1_binary = to_categorical(TargetHit1)\n",
        "\n",
        "# Going to scale each all of the data so want to find the mean and standard deviation\n",
        "ArmLength1Mean = np.mean(ArmLength1)\n",
        "ArmLength1std = np.std(ArmLength1)\n",
        "BallWeight1Mean = np.mean(BallWeight1)\n",
        "BallWeight1std = np.std(BallWeight1)\n",
        "BallRadius1Mean = np.mean(BallRadius1)\n",
        "BallRadius1std = np.std(BallRadius1)\n",
        "AirTemp1Mean = np.mean(AirTemp1)\n",
        "AirTemp1std = np.std(AirTemp1)\n",
        "SpringConst1Mean = np.mean(SpringConst1)\n",
        "SpringConst1std = np.std(SpringConst1)\n",
        "DeviceWeight1Mean = np.mean(DeviceWeight1)\n",
        "DeviceWeight1std = np.std(DeviceWeight1)\n",
        "\n",
        "# Finally creating the scaled arrays using these mean and std values\n",
        "ArmLength1Scaled = (ArmLength1 - ArmLength1Mean)/ArmLength1std\n",
        "BallWeight1Scaled = (BallWeight1 - BallWeight1Mean)/BallWeight1std\n",
        "BallRadius1Scaled = (BallRadius1 - BallRadius1Mean)/BallRadius1std\n",
        "AirTemp1Scaled = (AirTemp1 - AirTemp1Mean)/AirTemp1std\n",
        "SpringConst1Scaled = (SpringConst1 - SpringConst1Mean)/SpringConst1std\n",
        "DeviceWeight1Scaled = (DeviceWeight1 - DeviceWeight1Mean)/DeviceWeight1std\n",
        "\n",
        "# Finally forming these into one array to be parsed into the fit function\n",
        "Inputs = np.zeros((2000, 6))\n",
        "Inputs[:,0] = ArmLength1Scaled\n",
        "Inputs[:,1] = BallWeight1Scaled\n",
        "Inputs[:,2] = BallRadius1Scaled\n",
        "Inputs[:,3] = AirTemp1Scaled\n",
        "Inputs[:,4] = SpringConst1Scaled\n",
        "Inputs[:,5] = DeviceWeight1Scaled"
      ],
      "metadata": {
        "id": "95wNeH_peRL4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we have imported and processed our input data to be used to train the network"
      ],
      "metadata": {
        "id": "W6NruT16jp0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now will do the actual construction of the network"
      ],
      "metadata": {
        "id": "5leFeJ55jwwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising our neural network\n",
        "model1 = Sequential()\n",
        "\n",
        "# Adding layers, these layers are two layers of 4 nodes using the ReLU function and 1 with 2 nodes using the softmax function\n",
        "model1.add(Dense(units = 10, activation = 'relu', input_dim = 6))\n",
        "model1.add(Dense(units = 12, activation = 'relu'))\n",
        "model1.add(Dense(units = 6, activation = 'relu'))\n",
        "model1.add(Dense(units = 2, activation = 'softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model1.compile(loss = 'categorical_crossentropy', optimizer = 'sgd')"
      ],
      "metadata": {
        "id": "1DYPCcAJj00e"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the construction of the neural network, as we have 2000 points I think we can do a 5 split K fold cross validation and get good results so going to run that below in order to see what results the model gives and be able to work on improving it. This method is important to ensure that minimal overfitting is ocurring."
      ],
      "metadata": {
        "id": "QSszavOjxVe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up a k fold cross validation\n",
        "kf = KFold(n_splits = 5, shuffle = True)\n",
        "\n",
        "# Defining empty arrays to add our fraction values to so an average can be calculated at the end\n",
        "testfractions = 0\n",
        "trainfractions = 0\n",
        "a = 0;\n",
        "\n",
        "for train_index, test_index in kf.split(Inputs):\n",
        "  TrainingInputs = Inputs[train_index]\n",
        "  TrainingTargetHit = TargetHit1_binary[train_index]\n",
        "  TestInputs = Inputs[test_index]\n",
        "  TestTargetHit = TargetHit1_binary[test_index]\n",
        "\n",
        "  # Use X_train, y_train to train the SVM\n",
        "  model1.fit(TrainingInputs, TrainingTargetHit, epochs = 10, batch_size = 40)\n",
        "  # Use svm.predict() to predict the output for the test data set\n",
        "  testresults = model1.predict(TestInputs)\n",
        "  # loop through to compare the test data output to what it should be and obtain the fraction of correct classifications\n",
        "  count = 0\n",
        "  for i in range(len(TestTargetHit[:,0])):\n",
        "    if (testresults[i, 0] > testresults[i, 1] and TestTargetHit[i, 0] > TestTargetHit[i, 1]) or (testresults[i, 0] < testresults[i, 1] and TestTargetHit[i, 0] < TestTargetHit[i, 1]):\n",
        "      count = count+1\n",
        "  testfractions = testfractions + (count / len(TestTargetHit[:,0]))\n",
        "  # Do the same prediction and assessment performance with the training data\n",
        "  count = 0\n",
        "  trainresults = model1.predict(TrainingInputs)\n",
        "  for i in range(len(TrainingTargetHit[:,0])):\n",
        "    if (trainresults[i, 0] > trainresults[i, 1] and TrainingTargetHit[i, 0] > TrainingTargetHit[i, 1]) or (trainresults[i, 0] < trainresults[i, 1] and TrainingTargetHit[i, 0] < TrainingTargetHit[i, 1]):\n",
        "      count = count+1\n",
        "  trainfractions = trainfractions + (count / len(TrainingTargetHit[:,0]))\n",
        "\n",
        "# Dividing the sum of 5 fraction values to get the average\n",
        "testfraction = testfractions/5\n",
        "trainfraction = trainfractions/5\n",
        "\n",
        "# Printing the final accuracy values\n",
        "print(testfraction)\n",
        "print(trainfraction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O7TO3YBzDk3",
        "outputId": "371e8e06-0978-43fe-b2e5-d1390a8399f8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "40/40 [==============================] - 1s 2ms/step - loss: 0.6962\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6769\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6623\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6478\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6320\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.6147\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5952\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5736\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5500\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.5248\n",
            "13/13 [==============================] - 1s 40ms/step\n",
            "50/50 [==============================] - 2s 33ms/step\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.5006\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 26ms/step - loss: 0.4752\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4505\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4265\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.4043\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3833\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3641\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3468\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3309\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.3167\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "50/50 [==============================] - 0s 1ms/step\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 0.3038\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2931\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2840\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2759\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2691\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2632\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2581\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2538\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2499\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2463\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "50/50 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2504\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2475\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2448\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2426\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2407\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2382\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2366\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2349\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2337\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2321\n",
            "13/13 [==============================] - 1s 39ms/step\n",
            "50/50 [==============================] - 2s 38ms/step\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2211\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2191\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2176\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2160\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2146\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2134\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2122\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2111\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2103\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.2093\n",
            "13/13 [==============================] - 0s 38ms/step\n",
            "50/50 [==============================] - 0s 7ms/step\n",
            "0.8714999999999999\n",
            "0.8783749999999999\n"
          ]
        }
      ]
    }
  ]
}